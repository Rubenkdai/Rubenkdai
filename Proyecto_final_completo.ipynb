{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rubenkdai/Rubenkdai/blob/main/Proyecto_final_completo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b15054a",
      "metadata": {
        "id": "2b15054a"
      },
      "source": [
        "# **Detección de fraudes con tarjetas de crédito**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c044ce9",
      "metadata": {
        "id": "7c044ce9"
      },
      "source": [
        "En este proyecto, abordaremos el problema de la detección de fraudes con tarjetas de crédito utilizando un enfoque basado en Machine Learning. Usaremos un dataset público que contiene transacciones etiquetadas como fraudulentas o legítimas. El objetivo es construir un modelo que pueda identificar transacciones fraudulentas con alta precisión."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "801e38b8",
      "metadata": {
        "id": "801e38b8"
      },
      "source": [
        "## Carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a654745",
      "metadata": {
        "id": "7a654745"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Descargar el dataset desde Kaggle (o cargar un archivo local si está disponible)\n",
        "url = \"https://www.kaggleusercontent.com/mlg-ulb/creditcardfraud/creditcard.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Verificar las primeras filas\n",
        "df.head()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5769d7",
      "metadata": {
        "id": "4b5769d7"
      },
      "source": [
        "## Análisis exploratorio de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d729c5d0",
      "metadata": {
        "id": "d729c5d0"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Estadísticas descriptivas\n",
        "print(df.describe())\n",
        "\n",
        "# Distribución de clases\n",
        "sns.countplot(x='Class', data=df)\n",
        "plt.title(\"Distribución de clases\")\n",
        "plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c26226a",
      "metadata": {
        "id": "8c26226a"
      },
      "source": [
        "## Preprocesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43733fc9",
      "metadata": {
        "id": "43733fc9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separar características y etiquetas\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "# Dividir datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Escalar características\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b0d1334",
      "metadata": {
        "id": "4b0d1334"
      },
      "source": [
        "## Entrenamiento de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a5fb7e",
      "metadata": {
        "id": "b7a5fb7e"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Entrenar modelos\n",
        "log_reg = LogisticRegression(random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704bc47d",
      "metadata": {
        "id": "704bc47d"
      },
      "source": [
        "## Evaluación de los modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e41ad9",
      "metadata": {
        "id": "f8e41ad9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Evaluar el modelo de regresión logística\n",
        "y_pred_log = log_reg.predict(X_test)\n",
        "print(\"Resultados de Regresión Logística:\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(confusion_matrix(y_test, y_pred_log))\n",
        "\n",
        "# Evaluar el modelo Random Forest\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "print(\"Resultados de Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0791dcb7",
      "metadata": {
        "id": "0791dcb7"
      },
      "source": [
        "## Conclusiones"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52564651",
      "metadata": {
        "id": "52564651"
      },
      "source": [
        "En este proyecto, construimos y evaluamos modelos para la detección de fraudes con tarjetas de crédito. Se analizaron dos enfoques principales: Regresión Logística y Random Forest. Los resultados muestran que el modelo de Random Forest tiene mejor rendimiento para este conjunto de datos. Para mejorar aún más el modelo, podríamos considerar técnicas como:\n",
        "\n",
        "- Manejo de desbalance en las clases mediante sobremuestreo o submuestreo.\n",
        "- Uso de algoritmos más avanzados como Gradient Boosting.\n",
        "- Optimización de hiperparámetros."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}